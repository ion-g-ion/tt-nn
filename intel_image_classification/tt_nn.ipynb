{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as tn\n",
    "from torchvision import datasets, transforms\n",
    "import torchtt as tntt\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_name = 'cuda:0'\n",
    "data_dir_test = 'seg_test/'\n",
    "data_dir_train = 'seg_train/'\n",
    "N_shape = [15,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([transforms.Resize(N_shape[0]*N_shape[1]), transforms.CenterCrop(N_shape[0]*N_shape[1]), transforms.ToTensor()]) #, transforms.Normalize(tn.tensor([0.4885, 0.4525, 0.4163]), tn.tensor([0.2549, 0.2476, 0.2495]))])\n",
    "dataset_train = datasets.ImageFolder(data_dir_train, transform=transform_train)\n",
    "dataloader_train = tn.utils.data.DataLoader(dataset_train, batch_size=100, shuffle=True)\n",
    "\n",
    "transform_test = transforms.Compose([transforms.Resize(N_shape[0]*N_shape[1]), transforms.CenterCrop(N_shape[0]*N_shape[1]), transforms.ToTensor()]) #, transforms.Normalize(tn.tensor([0.4885, 0.4525, 0.4163]), tn.tensor([0.2549, 0.2476, 0.2495])) ])\n",
    "dataset_test = datasets.ImageFolder(data_dir_test, transform=transform_test)\n",
    "dataloader_test = tn.utils.data.DataLoader(dataset_test, batch_size=100, shuffle=True)\n",
    "\n",
    "# inputs_train = list(dataloader_train)[0][0].to(device_name)\n",
    "# labels_train = list(dataloader_train)[0][1].to(device_name)\n",
    "# \n",
    "# inputs_test = list(dataloader_test)[0][0].to(device_name)\n",
    "# labels_test = list(dataloader_test)[0][1].to(device_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicTT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ttl1 = tntt.nn.LinearLayerTT([3]+N_shape+N_shape, [10]+N_shape+N_shape, [1,6,2,2,2,1])\n",
    "        self.ttl2 = tntt.nn.LinearLayerTT([10]+N_shape+N_shape, [8,8,8,8,8], [1,2,2,2,2,1])\n",
    "        self.ttl3 = tntt.nn.LinearLayerTT([8,8,8,8,8], [4,4,4,4,4], [1,2,2,2,2,1])\n",
    "        self.ttl4 = tntt.nn.LinearLayerTT([4,4,4,4,4], [4,4,4,4,4], [1,2,2,2,2,1])\n",
    "        self.ttl5 = tntt.nn.LinearLayerTT([4,4,4,4,4], [3,3,3,3,3], [1,2,2,2,2,1])\n",
    "        self.linear = nn.Linear(3**5, 6, dtype = tn.float32)\n",
    "        self.logsoftmax = nn.LogSoftmax(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ttl1(x)\n",
    "        x = tn.relu(x)\n",
    "        x = self.ttl2(x)\n",
    "        x = tn.relu(x)\n",
    "        x = self.ttl3(x)\n",
    "        x = tn.relu(x)\n",
    "        x = self.ttl4(x)\n",
    "        x = tn.relu(x)\n",
    "        x = self.ttl5(x)\n",
    "        x = tn.relu(x)\n",
    "        x = x.view(-1,3**5)\n",
    "        x = self.linear(x)\n",
    "        return self.logsoftmax(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BasicTT()        \n",
    "model.to(device_name)\n",
    "\n",
    "# optimizer = tn.optim.SGD(model.parameters(), lr=0.0000001, momentum=0.9)\n",
    "optimizer = tn.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_function = tn.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_epoch(i):\n",
    "    \n",
    "    loss_total = 0.0\n",
    "    \n",
    "    for k, data in enumerate(dataloader_train):\n",
    "        # tme = datetime.datetime.now()\n",
    "        inputs, labels = data[0].to(device_name), data[1].to(device_name)\n",
    "        # tme = datetime.datetime.now() - tme\n",
    "        # print('t1',tme)\n",
    "        \n",
    "        # tme = datetime.datetime.now()\n",
    "        inputs = tn.reshape(inputs,[-1,3]+2*N_shape)\n",
    "        # tme = datetime.datetime.now() - tme\n",
    "        # print('t2',tme)\n",
    "        \n",
    "        # tme = datetime.datetime.now()\n",
    "        optimizer.zero_grad()\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(inputs)\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "        # tme = datetime.datetime.now() - tme\n",
    "        # print('t3',tme)\n",
    "        \n",
    "        loss_total += loss.item()\n",
    "        # print('\\t\\tbatch %d error %e'%(k+1,loss))\n",
    "    return loss_total/len(dataloader_train)\n",
    "\n",
    "def test_loss():\n",
    "    loss_total = 0 \n",
    "    for data in dataloader_test:\n",
    "        inputs, labels = data[0].to(device_name), data[1].to(device_name)\n",
    "        inputs = tn.reshape(inputs,[-1,3]+2*N_shape)\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss_total += loss.item()\n",
    "        \n",
    "    return loss_total/len(dataloader_test)\n",
    "\n",
    "def test_accuracy():\n",
    "    n_total = 0 \n",
    "    n_correct = 0\n",
    "    \n",
    "    for data in dataloader_test:\n",
    "        inputs, labels = data[0].to(device_name), data[1].to(device_name)\n",
    "        inputs = tn.reshape(inputs,[-1,3]+2*N_shape)\n",
    "        outputs = model(inputs)\n",
    "        n_correct += tn.sum(tn.max(outputs,1)[1] == labels)   \n",
    "        n_total+=inputs.shape[0]\n",
    "        \n",
    "    return n_correct/n_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "\n",
    "history_test = []\n",
    "history_train = []\n",
    "for epoch in range(n_epochs):\n",
    "    print('Epoch ',epoch+1)\n",
    "    time_epoch = datetime.datetime.now()\n",
    "    \n",
    "    model.train(True)\n",
    "    average_loss = do_epoch(epoch)\n",
    "    model.train(False)\n",
    "    \n",
    "    test_acc = test_accuracy()\n",
    "    time_epoch = datetime.datetime.now() - time_epoch\n",
    "    \n",
    "    print('\\tTraining loss %e test accuracy %5.4f'%(average_loss,test_acc))\n",
    "    print('\\tTime for the epoch',time_epoch)\n",
    "    history_test.append(test_acc)\n",
    "    history_train.append(average_loss)\n",
    "    \n",
    "plt.figure()\n",
    "plt.plot(np.arange(n_epochs)+1,np.array(history_train))\n",
    "plt.plot(np.arange(n_epochs)+1,np.array(history_test))\n",
    "plt.legend(['training','test'])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0bb18d59442045223691660a1f8e0079e69ab2417ab62beafa5ba9155c0a563f"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 ('pytorch2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
