{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as tn\n",
    "from torchvision import datasets, transforms\n",
    "import torchtt as tntt\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_name = 'cuda:0'\n",
    "data_dir_test = 'seg_test/'\n",
    "data_dir_train = 'seg_train/'\n",
    "N_shape = [15,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([transforms.Resize(N_shape[0]*N_shape[1]), transforms.CenterCrop(N_shape[0]*N_shape[1]), transforms.ToTensor()]) #, transforms.Normalize(tn.tensor([0.4885, 0.4525, 0.4163]), tn.tensor([0.2549, 0.2476, 0.2495]))])\n",
    "dataset_train = datasets.ImageFolder(data_dir_train, transform=transform_train)\n",
    "dataloader_train = tn.utils.data.DataLoader(dataset_train, batch_size=32, shuffle=True)\n",
    "\n",
    "transform_test = transforms.Compose([transforms.Resize(N_shape[0]*N_shape[1]), transforms.CenterCrop(N_shape[0]*N_shape[1]), transforms.ToTensor()]) #, transforms.Normalize(tn.tensor([0.4885, 0.4525, 0.4163]), tn.tensor([0.2549, 0.2476, 0.2495])) ])\n",
    "dataset_test = datasets.ImageFolder(data_dir_test, transform=transform_test)\n",
    "dataloader_test = tn.utils.data.DataLoader(dataset_test, batch_size=32, shuffle=True)\n",
    "\n",
    "# inputs_train = list(dataloader_train)[0][0].to(device_name)\n",
    "# labels_train = list(dataloader_train)[0][1].to(device_name)\n",
    "# \n",
    "# inputs_test = list(dataloader_test)[0][0].to(device_name)\n",
    "# labels_test = list(dataloader_test)[0][1].to(device_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicTT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        p = 0.2\n",
    "        self.ttl1 = tntt.nn.LinearLayerTT([3]+N_shape+N_shape, [16]+N_shape+N_shape, [1,4,2,2,2,1], initializer = 'He')\n",
    "        self.dropout1 = nn.Dropout(p)\n",
    "        self.ttl2 = tntt.nn.LinearLayerTT([16]+N_shape+N_shape, [16,8,8,8,8], [1,4,2,2,2,1], initializer = 'He')\n",
    "        self.dropout2 = nn.Dropout(p)\n",
    "        self.ttl3 = tntt.nn.LinearLayerTT([16,8,8,8,8], [8,4,4,4,4], [1,4,2,2,2,1], initializer = 'He')\n",
    "        self.dropout3 = nn.Dropout(p)\n",
    "        self.ttl4 = tntt.nn.LinearLayerTT([8,4,4,4,4], [3,3,3,3,3], [1,2,2,2,2,1], initializer = 'He')\n",
    "        self.dropout4 = nn.Dropout(p)\n",
    "        self.ttl5 = tntt.nn.LinearLayerTT([3,3,3,3,3], [3,3,3,3,3], [1,2,2,2,2,1], initializer = 'He')\n",
    "        self.dropout5 = nn.Dropout(p)\n",
    "        self.linear = nn.Linear(3**5, 6, dtype = tn.float32)\n",
    "        self.logsoftmax = nn.LogSoftmax(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ttl1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = tn.relu(x)\n",
    "        x = self.ttl2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = tn.relu(x)\n",
    "        x = self.ttl3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = tn.relu(x)\n",
    "        x = self.ttl4(x)\n",
    "        x = self.dropout4(x)\n",
    "        x = tn.relu(x)\n",
    "        x = self.ttl5(x)\n",
    "        x = self.dropout5(x)\n",
    "        x = tn.relu(x)\n",
    "        x = x.view(-1,3**5)\n",
    "        x = self.linear(x)\n",
    "        return self.logsoftmax(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BasicTT()        \n",
    "model.to(device_name)\n",
    "\n",
    "optimizer = tn.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "#optimizer = tn.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = tn.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.1)\n",
    "\n",
    "loss_function = tn.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_epoch(i):\n",
    "    \n",
    "    loss_total = 0.0\n",
    "    n_total = 0\n",
    "    n_correct = 0\n",
    "    #tme = datetime.datetime.now()\n",
    "    for k, data in enumerate(dataloader_train):\n",
    "        #tme = datetime.datetime.now() - tme\n",
    "        #print('t0',tme)\n",
    "        #tme = datetime.datetime.now()\n",
    "        inputs, labels = data[0].to(device_name), data[1].to(device_name)\n",
    "        #tme = datetime.datetime.now() - tme\n",
    "        #print('t1',tme)\n",
    "        \n",
    "        #tme = datetime.datetime.now()\n",
    "        inputs = tn.reshape(inputs,[-1,3]+2*N_shape)\n",
    "        #tme = datetime.datetime.now() - tme\n",
    "        #print('t2',tme)\n",
    "        \n",
    "        #tme = datetime.datetime.now()\n",
    "        optimizer.zero_grad()\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(inputs)\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_function(outputs, labels)\n",
    "        \n",
    "        # regularization\n",
    "        l2_lambda = 0.005\n",
    "        l2_norm = sum(p.pow(2.0).sum() for p in model.parameters())\n",
    "        loss = loss+l2_lambda*l2_norm\n",
    "        \n",
    "        loss.backward()\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "        #tme = datetime.datetime.now() - tme\n",
    "        #print('t3',tme)\n",
    "        n_correct += tn.sum(tn.max(outputs,1)[1] == labels).cpu()   \n",
    "        n_total+=inputs.shape[0]\n",
    "        \n",
    "        loss_total += loss.item()\n",
    "        # print('\\t\\tbatch %d error %e'%(k+1,loss))\n",
    "        #tme = datetime.datetime.now()\n",
    "        \n",
    "    return loss_total/len(dataloader_train), n_correct/n_total\n",
    "\n",
    "def test_loss():\n",
    "    loss_total = 0 \n",
    "    for data in dataloader_test:\n",
    "        inputs, labels = data[0].to(device_name), data[1].to(device_name)\n",
    "        inputs = tn.reshape(inputs,[-1,3]+2*N_shape)\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss_total += loss.item()\n",
    "        \n",
    "    return loss_total/len(dataloader_test)\n",
    "\n",
    "def test_data():\n",
    "    n_total = 0 \n",
    "    n_correct = 0\n",
    "    loss_total = 0\n",
    "    \n",
    "    for data in dataloader_test:\n",
    "        inputs, labels = data[0].to(device_name), data[1].to(device_name)\n",
    "        inputs = tn.reshape(inputs,[-1,3]+2*N_shape)\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss_total += loss.item()\n",
    "        n_correct += tn.sum(tn.max(outputs,1)[1] == labels)   \n",
    "        n_total+=inputs.shape[0]\n",
    "        \n",
    "    return loss_total/len(dataloader_test), n_correct/n_total\n",
    "\n",
    "def train_accuracy():\n",
    "    n_total = 0 \n",
    "    n_correct = 0\n",
    "    \n",
    "    for data in dataloader_train:\n",
    "        inputs, labels = data[0].to(device_name), data[1].to(device_name)\n",
    "        inputs = tn.reshape(inputs,[-1,3]+2*N_shape)\n",
    "        outputs = model(inputs)\n",
    "        n_correct += tn.sum(tn.max(outputs,1)[1] == labels)   \n",
    "        n_total+=inputs.shape[0]\n",
    "        \n",
    "    return n_correct/n_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\tTraining loss 4.077459e+00 training accuracy 0.2133 test loss 1.687261e+00 test accuracy 0.2817\n",
      "\tTime for the epoch 0:00:48.141242\n",
      "Epoch 2/200\n",
      "\tTraining loss 3.724290e+00 training accuracy 0.3335 test loss 1.432340e+00 test accuracy 0.4113\n",
      "\tTime for the epoch 0:00:47.871914\n",
      "Epoch 3/200\n",
      "\tTraining loss 3.411982e+00 training accuracy 0.4120 test loss 1.341239e+00 test accuracy 0.4540\n",
      "\tTime for the epoch 0:00:47.795931\n",
      "Epoch 4/200\n",
      "\tTraining loss 3.168022e+00 training accuracy 0.4613 test loss 1.219774e+00 test accuracy 0.5030\n",
      "\tTime for the epoch 0:00:47.894155\n",
      "Epoch 5/200\n",
      "\tTraining loss 2.968734e+00 training accuracy 0.4937 test loss 1.154517e+00 test accuracy 0.5257\n",
      "\tTime for the epoch 0:00:48.004484\n",
      "Epoch 6/200\n",
      "\tTraining loss 2.789260e+00 training accuracy 0.5327 test loss 1.094663e+00 test accuracy 0.5630\n",
      "\tTime for the epoch 0:00:48.059575\n",
      "Epoch 7/200\n",
      "\tTraining loss 2.630710e+00 training accuracy 0.5567 test loss 1.031080e+00 test accuracy 0.5893\n",
      "\tTime for the epoch 0:00:48.160177\n",
      "Epoch 8/200\n",
      "\tTraining loss 2.485727e+00 training accuracy 0.5843 test loss 1.003333e+00 test accuracy 0.5897\n",
      "\tTime for the epoch 0:00:48.054094\n",
      "Epoch 9/200\n",
      "\tTraining loss 2.356446e+00 training accuracy 0.5986 test loss 9.614151e-01 test accuracy 0.6237\n",
      "\tTime for the epoch 0:00:48.129811\n",
      "Epoch 10/200\n",
      "\tTraining loss 2.242958e+00 training accuracy 0.6171 test loss 9.351034e-01 test accuracy 0.6417\n",
      "\tTime for the epoch 0:00:48.152263\n",
      "Epoch 11/200\n",
      "\tTraining loss 2.142096e+00 training accuracy 0.6339 test loss 8.950981e-01 test accuracy 0.6477\n",
      "\tTime for the epoch 0:00:48.230187\n",
      "Epoch 12/200\n",
      "\tTraining loss 2.056694e+00 training accuracy 0.6449 test loss 8.739186e-01 test accuracy 0.6613\n",
      "\tTime for the epoch 0:00:48.054256\n",
      "Epoch 13/200\n",
      "\tTraining loss 1.978138e+00 training accuracy 0.6542 test loss 8.771491e-01 test accuracy 0.6653\n",
      "\tTime for the epoch 0:00:47.994673\n",
      "Epoch 14/200\n",
      "\tTraining loss 1.910331e+00 training accuracy 0.6579 test loss 8.482043e-01 test accuracy 0.6673\n",
      "\tTime for the epoch 0:00:47.872526\n",
      "Epoch 15/200\n",
      "\tTraining loss 1.836075e+00 training accuracy 0.6680 test loss 8.241757e-01 test accuracy 0.6830\n",
      "\tTime for the epoch 0:00:47.785346\n",
      "Epoch 16/200\n",
      "\tTraining loss 1.772147e+00 training accuracy 0.6815 test loss 8.199751e-01 test accuracy 0.6797\n",
      "\tTime for the epoch 0:00:47.812034\n",
      "Epoch 17/200\n",
      "\tTraining loss 1.717956e+00 training accuracy 0.6858 test loss 8.000767e-01 test accuracy 0.6887\n",
      "\tTime for the epoch 0:00:47.985142\n",
      "Epoch 18/200\n",
      "\tTraining loss 1.659080e+00 training accuracy 0.6963 test loss 8.095698e-01 test accuracy 0.6930\n",
      "\tTime for the epoch 0:00:48.276752\n",
      "Epoch 19/200\n",
      "\tTraining loss 1.609363e+00 training accuracy 0.7063 test loss 7.767392e-01 test accuracy 0.7003\n",
      "\tTime for the epoch 0:00:47.888062\n",
      "Epoch 20/200\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 200\n",
    "\n",
    "history_test_accuracy = []\n",
    "history_test_loss = []\n",
    "history_train_accuracy = []\n",
    "history_train_loss = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print('Epoch %d/%d'%(epoch+1,n_epochs))\n",
    "    \n",
    "    time_epoch = datetime.datetime.now()\n",
    "    \n",
    "    model.train(True)\n",
    "    train_loss, train_acc = do_epoch(epoch)\n",
    "    model.train(False)\n",
    "    \n",
    "    test_loss, test_acc = test_data()\n",
    "    scheduler.step()\n",
    "    \n",
    "    time_epoch = datetime.datetime.now() - time_epoch\n",
    "    \n",
    "    print('\\tTraining loss %e training accuracy %5.4f test loss %e test accuracy %5.4f'%(train_loss,train_acc,test_loss,test_acc))\n",
    "    print('\\tTime for the epoch',time_epoch)\n",
    "    history_test_accuracy.append(test_acc)\n",
    "    history_test_loss.append(test_loss)\n",
    "    history_train_accuracy.append(train_acc)\n",
    "    history_train_loss.append(train_loss)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(np.arange(len(history_train_accuracy))+1,np.array(history_train_accuracy))\n",
    "plt.plot(np.arange(len(history_test_accuracy))+1,np.array(history_test_accuracy))\n",
    "plt.plot(np.arange(len(history_train_accuracy))+1,np.array(history_train_accuracy))\n",
    "plt.plot(np.arange(len(history_train_loss))+1,np.array(history_train_loss))\n",
    "plt.legend(['training','test'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_test[:70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0bb18d59442045223691660a1f8e0079e69ab2417ab62beafa5ba9155c0a563f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
