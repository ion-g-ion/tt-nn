{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as tn\n",
    "from torchvision import datasets, transforms\n",
    "import torchtt as tntt\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_name = 'cuda:0'\n",
    "data_dir_test = 'seg_test/'\n",
    "data_dir_train = 'seg_train/'\n",
    "N_shape = [15,10]\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set torch.Size([14034, 3, 150, 150]) torch.Size([14034])\n",
      "Test set  torch.Size([3000, 3, 150, 150]) torch.Size([3000])\n"
     ]
    }
   ],
   "source": [
    "transform_train = transforms.Compose([transforms.Resize(N_shape[0]*N_shape[1]), transforms.CenterCrop(N_shape[0]*N_shape[1]), transforms.ToTensor()]) #, transforms.Normalize(tn.tensor([0.4885, 0.4525, 0.4163]), tn.tensor([0.2549, 0.2476, 0.2495]))])\n",
    "dataset_train = datasets.ImageFolder(data_dir_train, transform=transform_train)\n",
    "dataloader_train = tn.utils.data.DataLoader(dataset_train, batch_size=99999, shuffle=True, pin_memory = True, num_workers = 16)\n",
    "\n",
    "transform_test = transforms.Compose([transforms.Resize(N_shape[0]*N_shape[1]), transforms.CenterCrop(N_shape[0]*N_shape[1]), transforms.ToTensor()]) #, transforms.Normalize(tn.tensor([0.4885, 0.4525, 0.4163]), tn.tensor([0.2549, 0.2476, 0.2495])) ])\n",
    "dataset_test = datasets.ImageFolder(data_dir_test, transform=transform_test)\n",
    "dataloader_test = tn.utils.data.DataLoader(dataset_test, batch_size=99999, shuffle=True, pin_memory = True, num_workers = 16)\n",
    "\n",
    "inputs_train = list(dataloader_train)[0][0]\n",
    "labels_train = list(dataloader_train)[0][1]\n",
    " \n",
    "inputs_test = list(dataloader_test)[0][0]\n",
    "labels_test = list(dataloader_test)[0][1]\n",
    "\n",
    "print('Training set', inputs_train.shape,labels_train.shape)\n",
    "print('Test set ',inputs_test.shape, labels_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicTT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        p = 0.25\n",
    "        self.ttl1 = tntt.nn.LinearLayerTT([3]+N_shape+N_shape, [16]+N_shape+N_shape, [1,9,3,3,2,1], initializer = 'He')\n",
    "        self.dropout1 = nn.Dropout(p)\n",
    "        self.ttl2 = tntt.nn.LinearLayerTT([16]+N_shape+N_shape, [16,8,8,8,8], [1,4,2,2,2,1], initializer = 'He')\n",
    "        self.dropout2 = nn.Dropout(p)\n",
    "        self.ttl3 = tntt.nn.LinearLayerTT([16,8,8,8,8], [8,4,4,4,4], [1,4,2,2,2,1], initializer = 'He')\n",
    "        self.dropout3 = nn.Dropout(p)\n",
    "        self.ttl4 = tntt.nn.LinearLayerTT([8,4,4,4,4], [3,3,3,3,3], [1,2,2,2,2,1], initializer = 'He')\n",
    "        self.dropout4 = nn.Dropout(p)\n",
    "        self.ttl5 = tntt.nn.LinearLayerTT([3,3,3,3,3], [3,3,3,3,3], [1,2,2,2,2,1], initializer = 'He')\n",
    "        self.dropout5 = nn.Dropout(p)\n",
    "        self.linear = nn.Linear(3**5, 6, dtype = tn.float32)\n",
    "        self.logsoftmax = nn.LogSoftmax(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ttl1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = tn.relu(x)\n",
    "        x = self.ttl2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = tn.relu(x)\n",
    "        x = self.ttl3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = tn.relu(x)\n",
    "        x = self.ttl4(x)\n",
    "        x = self.dropout4(x)\n",
    "        x = tn.relu(x)\n",
    "        x = self.ttl5(x)\n",
    "        x = self.dropout5(x)\n",
    "        x = tn.relu(x)\n",
    "        x = x.view(-1,3**5)\n",
    "        x = self.linear(x)\n",
    "        return self.logsoftmax(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BasicTT()        \n",
    "model.to(device_name)\n",
    "\n",
    "optimizer = tn.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "#optimizer = tn.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = tn.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)\n",
    "\n",
    "loss_function = tn.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_epoch(i):\n",
    "    \n",
    "    loss_total = 0.0\n",
    "    n_total = 0\n",
    "    n_correct = 0\n",
    "    #tme = datetime.datetime.now()\n",
    "    n_batches = 0\n",
    "    \n",
    "    perm = tn.tensor(np.random.permutation(inputs_train.shape[0]))\n",
    "    perms = tn.tensor_split(perm,inputs_train.shape[0]//batch_size)\n",
    "        \n",
    "    for p in perms:\n",
    "        #tme = datetime.datetime.now() - tme\n",
    "        #print('t0',tme)\n",
    "        # tme = datetime.datetime.now()\n",
    "        inputs = inputs_train[p,:,:,:].to(device_name)\n",
    "        labels = labels_train[p].to(device_name)\n",
    "\n",
    "        # tme = datetime.datetime.now() - tme\n",
    "        # print('t1',tme)\n",
    "        \n",
    "        # tme = datetime.datetime.now()\n",
    "        inputs = tn.reshape(inputs,[-1,3]+2*N_shape)\n",
    "        # tme = datetime.datetime.now() - tme\n",
    "        # print('t2',tme)\n",
    "        \n",
    "        # tme = datetime.datetime.now()\n",
    "        optimizer.zero_grad()\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(inputs)\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_function(outputs, labels)\n",
    "        \n",
    "        # regularization\n",
    "        l2_lambda = 0.005\n",
    "        l2_norm = sum(p.pow(2.0).sum() for p in model.parameters())\n",
    "        loss = loss+l2_lambda*l2_norm\n",
    "        \n",
    "        loss.backward()\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "        # tme = datetime.datetime.now() - tme\n",
    "        # print('t3',tme)\n",
    "        n_correct += tn.sum(tn.max(outputs,1)[1] == labels).cpu()   \n",
    "        n_total+=inputs.shape[0]\n",
    "        \n",
    "        loss_total += loss.item()\n",
    "        n_batches+=1\n",
    "        # print('\\t\\tbatch %d error %e'%(k+1,loss))\n",
    "        #tme = datetime.datetime.now()\n",
    "        \n",
    "    return loss_total/n_batches, n_correct/n_total\n",
    "\n",
    "\n",
    "\n",
    "def test_data():\n",
    "    n_total = 0 \n",
    "    n_correct = 0\n",
    "    loss_total = 0\n",
    "    \n",
    "    perm = tn.tensor(np.random.permutation(inputs_test.shape[0]))\n",
    "    perms = tn.tensor_split(perm,inputs_test.shape[0]//batch_size)\n",
    "        \n",
    "        \n",
    "    for p in perms:\n",
    "        inputs = inputs_test[p,:,:,:].to(device_name)\n",
    "        labels = labels_test[p].to(device_name)\n",
    "        inputs = tn.reshape(inputs,[-1,3]+2*N_shape)\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss_total += loss.item()\n",
    "        n_correct += tn.sum(tn.max(outputs,1)[1] == labels)   \n",
    "        n_total+=inputs.shape[0]\n",
    "        \n",
    "    return loss_total/len(dataloader_test), n_correct/n_total\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\tTraining loss 4.088508e+00 training accuracy 0.1746 test loss 1.665319e+02 test accuracy 0.1713\n",
      "\tTime for the epoch 0:00:15.883593\n",
      "Epoch 2/200\n",
      "\tTraining loss 3.882070e+00 training accuracy 0.1765 test loss 1.665091e+02 test accuracy 0.1750\n",
      "\tTime for the epoch 0:00:14.362440\n",
      "Epoch 3/200\n",
      "\tTraining loss 3.705994e+00 training accuracy 0.1785 test loss 1.664987e+02 test accuracy 0.1750\n",
      "\tTime for the epoch 0:00:14.248921\n",
      "Epoch 4/200\n",
      "\tTraining loss 3.545270e+00 training accuracy 0.1790 test loss 1.664954e+02 test accuracy 0.1750\n",
      "\tTime for the epoch 0:00:14.853008\n",
      "Epoch 5/200\n",
      "\tTraining loss 3.397970e+00 training accuracy 0.1790 test loss 1.664972e+02 test accuracy 0.1750\n",
      "\tTime for the epoch 0:00:14.744421\n",
      "Epoch 6/200\n",
      "\tTraining loss 3.263061e+00 training accuracy 0.1790 test loss 1.664878e+02 test accuracy 0.1750\n",
      "\tTime for the epoch 0:00:15.099879\n",
      "Epoch 7/200\n",
      "\tTraining loss 3.139448e+00 training accuracy 0.1790 test loss 1.664867e+02 test accuracy 0.1750\n",
      "\tTime for the epoch 0:00:14.368011\n",
      "Epoch 8/200\n",
      "\tTraining loss 3.026265e+00 training accuracy 0.1790 test loss 1.664845e+02 test accuracy 0.1750\n",
      "\tTime for the epoch 0:00:13.682929\n",
      "Epoch 9/200\n",
      "\tTraining loss 2.922551e+00 training accuracy 0.1790 test loss 1.664878e+02 test accuracy 0.1750\n",
      "\tTime for the epoch 0:00:13.900923\n",
      "Epoch 10/200\n",
      "\tTraining loss 2.827556e+00 training accuracy 0.1790 test loss 1.664968e+02 test accuracy 0.1750\n",
      "\tTime for the epoch 0:00:14.775674\n",
      "Epoch 11/200\n",
      "\tTraining loss 2.740529e+00 training accuracy 0.1790 test loss 1.664897e+02 test accuracy 0.1750\n",
      "\tTime for the epoch 0:00:14.448761\n",
      "Epoch 12/200\n",
      "\tTraining loss 2.660818e+00 training accuracy 0.1790 test loss 1.664842e+02 test accuracy 0.1750\n",
      "\tTime for the epoch 0:00:14.163585\n",
      "Epoch 13/200\n",
      "\tTraining loss 2.587784e+00 training accuracy 0.1790 test loss 1.664892e+02 test accuracy 0.1750\n",
      "\tTime for the epoch 0:00:14.316310\n",
      "Epoch 14/200\n",
      "\tTraining loss 2.520876e+00 training accuracy 0.1790 test loss 1.664847e+02 test accuracy 0.1750\n",
      "\tTime for the epoch 0:00:14.103346\n",
      "Epoch 15/200\n",
      "\tTraining loss 2.459594e+00 training accuracy 0.1790 test loss 1.664859e+02 test accuracy 0.1750\n",
      "\tTime for the epoch 0:00:14.229773\n",
      "Epoch 16/200\n",
      "\tTraining loss 2.403457e+00 training accuracy 0.1790 test loss 1.664885e+02 test accuracy 0.1750\n",
      "\tTime for the epoch 0:00:14.146546\n",
      "Epoch 17/200\n",
      "\tTraining loss 2.352035e+00 training accuracy 0.1790 test loss 1.664884e+02 test accuracy 0.1750\n",
      "\tTime for the epoch 0:00:14.264119\n",
      "Epoch 18/200\n",
      "\tTraining loss 2.304940e+00 training accuracy 0.1790 test loss 1.664841e+02 test accuracy 0.1750\n",
      "\tTime for the epoch 0:00:14.555617\n",
      "Epoch 19/200\n",
      "\tTraining loss 2.261798e+00 training accuracy 0.1790 test loss 1.664880e+02 test accuracy 0.1750\n",
      "\tTime for the epoch 0:00:14.179548\n",
      "Epoch 20/200\n",
      "\tTraining loss 2.222250e+00 training accuracy 0.1790 test loss 1.664890e+02 test accuracy 0.1750\n",
      "\tTime for the epoch 0:00:14.444649\n",
      "Epoch 21/200\n",
      "\tTraining loss 2.186057e+00 training accuracy 0.1790 test loss 1.664843e+02 test accuracy 0.1750\n",
      "\tTime for the epoch 0:00:13.769566\n",
      "Epoch 22/200\n",
      "\tTraining loss 2.152887e+00 training accuracy 0.1790 test loss 1.664922e+02 test accuracy 0.1750\n",
      "\tTime for the epoch 0:00:14.800181\n",
      "Epoch 23/200\n",
      "\tTraining loss 2.122500e+00 training accuracy 0.1790 test loss 1.664898e+02 test accuracy 0.1750\n",
      "\tTime for the epoch 0:00:14.567852\n",
      "Epoch 24/200\n",
      "\tTraining loss 2.094664e+00 training accuracy 0.1790 test loss 1.664864e+02 test accuracy 0.1750\n",
      "\tTime for the epoch 0:00:14.415494\n",
      "Epoch 25/200\n",
      "\tTraining loss 2.069169e+00 training accuracy 0.1790 test loss 1.664854e+02 test accuracy 0.1750\n",
      "\tTime for the epoch 0:00:14.305472\n",
      "Epoch 26/200\n",
      "\tTraining loss 2.045782e+00 training accuracy 0.1790 test loss 1.664905e+02 test accuracy 0.1750\n",
      "\tTime for the epoch 0:00:14.290122\n",
      "Epoch 27/200\n",
      "\tTraining loss 2.024393e+00 training accuracy 0.1790 test loss 1.664901e+02 test accuracy 0.1750\n",
      "\tTime for the epoch 0:00:13.896855\n",
      "Epoch 28/200\n",
      "\tTraining loss 2.004820e+00 training accuracy 0.1790 test loss 1.664906e+02 test accuracy 0.1750\n",
      "\tTime for the epoch 0:00:14.139322\n",
      "Epoch 29/200\n",
      "\tTraining loss 1.986852e+00 training accuracy 0.1790 test loss 1.664912e+02 test accuracy 0.1750\n",
      "\tTime for the epoch 0:00:14.129285\n",
      "Epoch 30/200\n",
      "\tTraining loss 1.970392e+00 training accuracy 0.1790 test loss 1.664937e+02 test accuracy 0.1750\n",
      "\tTime for the epoch 0:00:14.355815\n",
      "Epoch 31/200\n",
      "\tTraining loss 1.955332e+00 training accuracy 0.1790 test loss 1.664860e+02 test accuracy 0.1750\n",
      "\tTime for the epoch 0:00:13.917924\n",
      "Epoch 32/200\n",
      "\tTraining loss 1.941510e+00 training accuracy 0.1790 test loss 1.664843e+02 test accuracy 0.1750\n",
      "\tTime for the epoch 0:00:13.968327\n",
      "Epoch 33/200\n",
      "\tTraining loss 1.928896e+00 training accuracy 0.1790 test loss 1.664807e+02 test accuracy 0.1750\n",
      "\tTime for the epoch 0:00:13.915589\n",
      "Epoch 34/200\n",
      "\tTraining loss 1.917291e+00 training accuracy 0.1790 test loss 1.664795e+02 test accuracy 0.1750\n",
      "\tTime for the epoch 0:00:14.043992\n",
      "Epoch 35/200\n",
      "\tTraining loss 1.906681e+00 training accuracy 0.1790 test loss 1.664879e+02 test accuracy 0.1750\n",
      "\tTime for the epoch 0:00:14.077637\n",
      "Epoch 36/200\n",
      "\tTraining loss 1.896980e+00 training accuracy 0.1790 test loss 1.664852e+02 test accuracy 0.1750\n",
      "\tTime for the epoch 0:00:14.377385\n",
      "Epoch 37/200\n",
      "\tTraining loss 1.888081e+00 training accuracy 0.1790 test loss 1.664877e+02 test accuracy 0.1750\n",
      "\tTime for the epoch 0:00:14.016941\n",
      "Epoch 38/200\n",
      "\tTraining loss 1.879900e+00 training accuracy 0.1790 test loss 1.664913e+02 test accuracy 0.1750\n",
      "\tTime for the epoch 0:00:13.874917\n",
      "Epoch 39/200\n",
      "\tTraining loss 1.872444e+00 training accuracy 0.1790 test loss 1.664769e+02 test accuracy 0.1750\n",
      "\tTime for the epoch 0:00:14.196358\n",
      "Epoch 40/200\n",
      "\tTraining loss 1.865618e+00 training accuracy 0.1790 test loss 1.664783e+02 test accuracy 0.1750\n",
      "\tTime for the epoch 0:00:14.395451\n",
      "Epoch 41/200\n",
      "\tTraining loss 1.859314e+00 training accuracy 0.1790 test loss 1.664901e+02 test accuracy 0.1750\n",
      "\tTime for the epoch 0:00:14.180350\n",
      "Epoch 42/200\n",
      "\tTraining loss 1.853592e+00 training accuracy 0.1790 test loss 1.664908e+02 test accuracy 0.1750\n",
      "\tTime for the epoch 0:00:13.955636\n",
      "Epoch 43/200\n",
      "\tTraining loss 1.848324e+00 training accuracy 0.1790 test loss 1.664877e+02 test accuracy 0.1750\n",
      "\tTime for the epoch 0:00:14.185578\n",
      "Epoch 44/200\n",
      "\tTraining loss 1.843504e+00 training accuracy 0.1790 test loss 1.664878e+02 test accuracy 0.1750\n",
      "\tTime for the epoch 0:00:13.921597\n",
      "Epoch 45/200\n",
      "\tTraining loss 1.839094e+00 training accuracy 0.1790 test loss 1.664888e+02 test accuracy 0.1750\n",
      "\tTime for the epoch 0:00:14.363667\n",
      "Epoch 46/200\n",
      "\tTraining loss 1.835023e+00 training accuracy 0.1790 test loss 1.664801e+02 test accuracy 0.1750\n",
      "\tTime for the epoch 0:00:14.096006\n",
      "Epoch 47/200\n",
      "\tTraining loss 1.831353e+00 training accuracy 0.1790 test loss 1.664815e+02 test accuracy 0.1750\n",
      "\tTime for the epoch 0:00:13.819492\n",
      "Epoch 48/200\n",
      "\tTraining loss 1.827944e+00 training accuracy 0.1790 test loss 1.664907e+02 test accuracy 0.1750\n",
      "\tTime for the epoch 0:00:14.330759\n",
      "Epoch 49/200\n",
      "\tTraining loss 1.824855e+00 training accuracy 0.1790 test loss 1.664911e+02 test accuracy 0.1750\n",
      "\tTime for the epoch 0:00:14.748778\n",
      "Epoch 50/200\n",
      "\tTraining loss 1.821986e+00 training accuracy 0.1790 test loss 1.664778e+02 test accuracy 0.1750\n",
      "\tTime for the epoch 0:00:14.725898\n",
      "Epoch 51/200\n",
      "\tTraining loss 1.819966e+00 training accuracy 0.1790 test loss 1.664844e+02 test accuracy 0.1750\n",
      "\tTime for the epoch 0:00:14.150809\n",
      "Epoch 52/200\n",
      "\tTraining loss 1.818699e+00 training accuracy 0.1790 test loss 1.664817e+02 test accuracy 0.1750\n",
      "\tTime for the epoch 0:00:14.323736\n",
      "Epoch 53/200\n",
      "\tTraining loss 1.817517e+00 training accuracy 0.1790 test loss 1.664849e+02 test accuracy 0.1750\n",
      "\tTime for the epoch 0:00:14.307585\n",
      "Epoch 54/200\n",
      "\tTraining loss 1.816370e+00 training accuracy 0.1790 test loss 1.664868e+02 test accuracy 0.1750\n",
      "\tTime for the epoch 0:00:13.994997\n",
      "Epoch 55/200\n",
      "\tTraining loss 1.815267e+00 training accuracy 0.1790 test loss 1.664871e+02 test accuracy 0.1750\n",
      "\tTime for the epoch 0:00:13.925708\n",
      "Epoch 56/200\n",
      "\tTraining loss 1.814227e+00 training accuracy 0.1790 test loss 1.664871e+02 test accuracy 0.1750\n",
      "\tTime for the epoch 0:00:14.017227\n",
      "Epoch 57/200\n",
      "\tTraining loss 1.813236e+00 training accuracy 0.1790 test loss 1.664806e+02 test accuracy 0.1750\n",
      "\tTime for the epoch 0:00:14.521257\n",
      "Epoch 58/200\n",
      "\tTraining loss 1.812266e+00 training accuracy 0.1790 test loss 1.664875e+02 test accuracy 0.1750\n",
      "\tTime for the epoch 0:00:14.161609\n",
      "Epoch 59/200\n",
      "\tTraining loss 1.811351e+00 training accuracy 0.1790 test loss 1.664889e+02 test accuracy 0.1750\n",
      "\tTime for the epoch 0:00:14.542449\n",
      "Epoch 60/200\n",
      "\tTraining loss 1.810465e+00 training accuracy 0.1790 test loss 1.664881e+02 test accuracy 0.1750\n",
      "\tTime for the epoch 0:00:14.160271\n",
      "Epoch 61/200\n",
      "\tTraining loss 1.809626e+00 training accuracy 0.1790 test loss 1.664873e+02 test accuracy 0.1750\n",
      "\tTime for the epoch 0:00:14.183044\n",
      "Epoch 62/200\n",
      "\tTraining loss 1.808812e+00 training accuracy 0.1790 test loss 1.664894e+02 test accuracy 0.1750\n",
      "\tTime for the epoch 0:00:14.005928\n",
      "Epoch 63/200\n",
      "\tTraining loss 1.808058e+00 training accuracy 0.1790 test loss 1.664859e+02 test accuracy 0.1750\n",
      "\tTime for the epoch 0:00:14.085755\n",
      "Epoch 64/200\n",
      "\tTraining loss 1.807327e+00 training accuracy 0.1790 test loss 1.664843e+02 test accuracy 0.1750\n",
      "\tTime for the epoch 0:00:13.972593\n",
      "Epoch 65/200\n",
      "\tTraining loss 1.806619e+00 training accuracy 0.1790 test loss 1.664853e+02 test accuracy 0.1750\n",
      "\tTime for the epoch 0:00:14.005370\n",
      "Epoch 66/200\n",
      "\tTraining loss 1.805939e+00 training accuracy 0.1790 test loss 1.664869e+02 test accuracy 0.1750\n",
      "\tTime for the epoch 0:00:14.021509\n",
      "Epoch 67/200\n",
      "\tTraining loss 1.805308e+00 training accuracy 0.1790 test loss 1.664893e+02 test accuracy 0.1750\n",
      "\tTime for the epoch 0:00:14.139400\n",
      "Epoch 68/200\n",
      "\tTraining loss 1.804671e+00 training accuracy 0.1790 test loss 1.664889e+02 test accuracy 0.1750\n",
      "\tTime for the epoch 0:00:14.015717\n",
      "Epoch 69/200\n",
      "\tTraining loss 1.804085e+00 training accuracy 0.1790 test loss 1.664890e+02 test accuracy 0.1750\n",
      "\tTime for the epoch 0:00:14.197710\n",
      "Epoch 70/200\n",
      "\tTraining loss 1.803514e+00 training accuracy 0.1790 test loss 1.664875e+02 test accuracy 0.1750\n",
      "\tTime for the epoch 0:00:14.075823\n",
      "Epoch 71/200\n",
      "\tTraining loss 1.802976e+00 training accuracy 0.1790 test loss 1.664893e+02 test accuracy 0.1750\n",
      "\tTime for the epoch 0:00:12.777420\n",
      "Epoch 72/200\n",
      "\tTraining loss 1.802458e+00 training accuracy 0.1790 test loss 1.664850e+02 test accuracy 0.1750\n",
      "\tTime for the epoch 0:00:12.811941\n",
      "Epoch 73/200\n",
      "\tTraining loss 1.801953e+00 training accuracy 0.1790 test loss 1.664864e+02 test accuracy 0.1750\n",
      "\tTime for the epoch 0:00:12.781807\n",
      "Epoch 74/200\n",
      "\tTraining loss 1.801472e+00 training accuracy 0.1790 test loss 1.664856e+02 test accuracy 0.1750\n",
      "\tTime for the epoch 0:00:13.602218\n",
      "Epoch 75/200\n",
      "\tTraining loss 1.801016e+00 training accuracy 0.1790 test loss 1.664871e+02 test accuracy 0.1750\n",
      "\tTime for the epoch 0:00:14.447729\n",
      "Epoch 76/200\n",
      "\tTraining loss 1.800589e+00 training accuracy 0.1790 test loss 1.664898e+02 test accuracy 0.1750\n",
      "\tTime for the epoch 0:00:14.309623\n",
      "Epoch 77/200\n",
      "\tTraining loss 1.800167e+00 training accuracy 0.1790 test loss 1.664910e+02 test accuracy 0.1750\n",
      "\tTime for the epoch 0:00:14.812280\n",
      "Epoch 78/200\n",
      "\tTraining loss 1.799764e+00 training accuracy 0.1790 test loss 1.664908e+02 test accuracy 0.1750\n",
      "\tTime for the epoch 0:00:14.346415\n",
      "Epoch 79/200\n",
      "\tTraining loss 1.799392e+00 training accuracy 0.1790 test loss 1.664925e+02 test accuracy 0.1750\n",
      "\tTime for the epoch 0:00:15.280791\n",
      "Epoch 80/200\n",
      "\tTraining loss 1.799021e+00 training accuracy 0.1790 test loss 1.664919e+02 test accuracy 0.1750\n",
      "\tTime for the epoch 0:00:14.142654\n",
      "Epoch 81/200\n",
      "\tTraining loss 1.798677e+00 training accuracy 0.1790 test loss 1.664900e+02 test accuracy 0.1750\n",
      "\tTime for the epoch 0:00:14.338589\n",
      "Epoch 82/200\n",
      "\tTraining loss 1.798340e+00 training accuracy 0.1790 test loss 1.664858e+02 test accuracy 0.1750\n",
      "\tTime for the epoch 0:00:15.233667\n",
      "Epoch 83/200\n",
      "\tTraining loss 1.798019e+00 training accuracy 0.1790 test loss 1.664879e+02 test accuracy 0.1750\n",
      "\tTime for the epoch 0:00:15.610901\n",
      "Epoch 84/200\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-1c57841b6055>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdo_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-9065ff485219>\u001b[0m in \u001b[0;36mdo_epoch\u001b[0;34m(i)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# Make predictions for this batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;31m# Compute the loss and its gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch2/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1105\u001b[0;31m         \u001b[0mforward_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1106\u001b[0m         \u001b[0;31m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m         \u001b[0;31m# this function, and just call forward.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_epochs = 200\n",
    "\n",
    "history_test_accuracy = []\n",
    "history_test_loss = []\n",
    "history_train_accuracy = []\n",
    "history_train_loss = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print('Epoch %d/%d'%(epoch+1,n_epochs))\n",
    "    \n",
    "    time_epoch = datetime.datetime.now()\n",
    "    \n",
    "    model.train(True)\n",
    "    train_loss, train_acc = do_epoch(epoch)\n",
    "    model.train(False)\n",
    "    \n",
    "    test_loss, test_acc = test_data()\n",
    "    scheduler.step()\n",
    "    \n",
    "    time_epoch = datetime.datetime.now() - time_epoch\n",
    "    \n",
    "    print('\\tTraining loss %e training accuracy %5.4f test loss %e test accuracy %5.4f'%(train_loss,train_acc,test_loss,test_acc))\n",
    "    print('\\tTime for the epoch',time_epoch)\n",
    "    history_test_accuracy.append(test_acc)\n",
    "    history_test_loss.append(test_loss)\n",
    "    history_train_accuracy.append(train_acc)\n",
    "    history_train_loss.append(train_loss)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(np.arange(len(history_train_accuracy))+1,np.array(history_train_accuracy))\n",
    "plt.plot(np.arange(len(history_test_accuracy))+1,np.array(history_test_accuracy))\n",
    "plt.legend(['training','test'])\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.arange(len(history_train_loss))+1,np.array(history_train_loss))\n",
    "plt.plot(np.arange(len(history_test_loss))+1,np.array(history_test_loss))\n",
    "plt.legend(['training','test'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_train.shape[0]//batch_size\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0bb18d59442045223691660a1f8e0079e69ab2417ab62beafa5ba9155c0a563f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
